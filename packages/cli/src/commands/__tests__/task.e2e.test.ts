/**
 * ============================================================================
 * AUTOGENERATED E2E TEST FILE - DO NOT MODIFY MANUALLY
 * ============================================================================
 *
 * This file is automatically generated based on the E2E test specifications.
 * Any manual changes will be overwritten by the automation.
 *
 * To modify these tests, update the specification files:
 * - /docs/E2E_TESTS.md (root-level testing philosophy and constraints)
 * - /packages/cli/E2E_TESTS.md (CLI-specific test specifications)
 *
 * Then run the /update-e2e-tests command to regenerate this file.
 * ============================================================================
 */

import { beforeEach, afterEach, describe, it, expect } from 'vitest';
import { SKIP_REAL_AGENT_TESTS } from './e2e-utils.js';
import {
  mkdtempSync,
  rmSync,
  writeFileSync,
  readFileSync,
  existsSync,
  mkdirSync,
  chmodSync,
} from 'node:fs';
import { tmpdir } from 'node:os';
import { join } from 'node:path';
import { execa } from 'execa';

/**
 * E2E tests for `rover task` command
 *
 * These tests run the actual rover CLI binary and test the full task execution workflow.
 * They mock system tool availability by creating wrapper scripts in a temporary bin directory.
 */

describe('rover task (e2e)', () => {
  let testDir: string;
  let originalCwd: string;
  let mockBinDir: string;
  let mockHomeDir: string;
  let originalPath: string;

  /**
   * Creates a mock executable in the mock bin directory
   * This allows us to control which tools appear "installed" to the rover CLI
   */
  const createMockTool = (
    toolName: string,
    exitCode: number = 0,
    output: string = 'mock version 1.0.0'
  ) => {
    const scriptPath = join(mockBinDir, toolName);
    const scriptContent = `#!/usr/bin/env bash\necho "${output}"\nexit ${exitCode}`;
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  /**
   * Creates a mock executable from a full script string
   */
  const createMockScript = (toolName: string, scriptContent: string) => {
    const scriptPath = join(mockBinDir, toolName);
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  /**
   * Creates a mock Claude CLI that handles both --version checks and
   * task expansion invocations (claude -p --output-format json).
   * The mock returns a valid JSON response for expandTask.
   */
  const createMockClaude = () => {
    createMockScript(
      'claude',
      `#!/usr/bin/env bash
if [[ "$1" == "--version" ]]; then
  echo "Claude CLI v1.0.0"
  exit 0
fi

# Handle prompt mode: claude -p [--output-format json]
if [[ "$1" == "-p" ]]; then
  # Read stdin (the prompt)
  PROMPT=$(cat)

  # Check if JSON output format is requested
  if [[ "$2" == "--output-format" && "$3" == "json" ]]; then
    # Return a valid Claude JSON response with nested result
    echo '{"result":"{\\"title\\":\\"Create hello world bash script\\",\\"description\\":\\"Create a hello world bash script that prints Hello World and the current date and time.\\"}"}'
  else
    echo '{"title":"Create hello world bash script","description":"Create a hello world bash script that prints Hello World."}'
  fi
  exit 0
fi

echo "Claude CLI v1.0.0"
exit 0
`
    );
  };

  beforeEach(async () => {
    // Save original state
    originalCwd = process.cwd();
    originalPath = process.env.PATH || '';

    // Create temporary test directory
    testDir = mkdtempSync(join(tmpdir(), 'rover-task-e2e-'));
    process.chdir(testDir);

    // Create mock bin directory for mocking system tools
    mockBinDir = join(testDir, '.mock-bin');
    mkdirSync(mockBinDir, { recursive: true });

    // Create mock HOME directory with agent configuration files
    // This is necessary because the CLI validates that agent configs exist
    mockHomeDir = join(testDir, '.mock-home');
    mkdirSync(mockHomeDir, { recursive: true });

    // Create mock Claude config file (required by task command validation)
    writeFileSync(
      join(mockHomeDir, '.claude.json'),
      JSON.stringify({ version: 1 })
    );

    // Create mock Gemini config files (for --agent gemini tests)
    const geminiDir = join(mockHomeDir, '.gemini');
    mkdirSync(geminiDir, { recursive: true });
    writeFileSync(
      join(geminiDir, 'settings.json'),
      JSON.stringify({ version: 1 })
    );
    writeFileSync(
      join(geminiDir, 'oauth_creds.json'),
      JSON.stringify({ token: 'mock' })
    );

    // Create failing mocks for all tools by default
    // This ensures that only explicitly enabled tools will be detected
    createMockTool('docker', 127, 'command not found: docker');
    createMockTool('claude', 127, 'command not found: claude');
    createMockTool('codex', 127, 'command not found: codex');
    createMockTool('cursor', 127, 'command not found: cursor');
    createMockTool('cursor-agent', 127, 'command not found: cursor-agent');
    createMockTool('gemini', 127, 'command not found: gemini');
    createMockTool('qwen', 127, 'command not found: qwen');
    createMockTool('opencode', 127, 'command not found: opencode');

    // Enable Docker and Claude for the initialization and task phases
    createMockTool('docker', 0, 'Docker version 24.0.0');
    createMockClaude();

    // Prepend mock bin to PATH so our mock tools are found first
    process.env.PATH = `${mockBinDir}:${originalPath}`;

    // Initialize a real git repository
    await execa('git', ['init']);
    await execa('git', ['config', 'user.email', 'test@test.com']);
    await execa('git', ['config', 'user.name', 'Test User']);
    await execa('git', ['config', 'commit.gpgsign', 'false']);

    // Create initial project files
    writeFileSync(
      'package.json',
      JSON.stringify(
        {
          name: 'test-project',
          version: '1.0.0',
          type: 'module',
        },
        null,
        2
      )
    );
    writeFileSync('README.md', '# Test Project\n');

    // Create an initial commit
    await execa('git', ['add', '.']);
    await execa('git', ['commit', '-m', 'Initial commit']);

    // Initialize rover
    const roverBin = join(__dirname, '../../../dist/index.mjs');
    const testPath = `${mockBinDir}:${originalPath}`;

    await execa('node', [roverBin, 'init', '--yes'], {
      cwd: testDir,
      env: {
        PATH: testPath,
        HOME: mockHomeDir,
        USER: process.env.USER,
        TMPDIR: process.env.TMPDIR,
        ROVER_NO_TELEMETRY: '1',
      },
    });
  });

  afterEach(() => {
    // Restore original state
    process.chdir(originalCwd);
    process.env.PATH = originalPath;
    rmSync(testDir, { recursive: true, force: true });
  });

  /**
   * Helper to run the rover task command
   */
  const runRoverTask = async (taskDescription: string, args: string[] = []) => {
    const roverBin = join(__dirname, '../../../dist/index.mjs');
    const testPath = `${mockBinDir}:${originalPath}`;

    return execa('node', [roverBin, 'task', '-y', taskDescription, ...args], {
      cwd: testDir,
      env: {
        PATH: testPath,
        HOME: mockHomeDir,
        USER: process.env.USER,
        TMPDIR: process.env.TMPDIR,
        ROVER_NO_TELEMETRY: '1',
      },
      reject: false, // Don't throw on non-zero exit
    });
  };

  /**
   * Helper to wait for a task to reach a specific status
   * Polls the task status file until the expected status is reached or timeout occurs
   */
  const waitForTaskStatus = async (
    taskId: number,
    expectedStatus: string,
    timeoutMs: number = 30000,
    pollIntervalMs: number = 500
  ): Promise<void> => {
    const startTime = Date.now();
    const taskStatusFile = join(
      testDir,
      `.rover/tasks/${taskId}/description.json`
    );

    while (Date.now() - startTime < timeoutMs) {
      if (existsSync(taskStatusFile)) {
        const statusContent = readFileSync(taskStatusFile, 'utf8');
        const status = JSON.parse(statusContent);

        if (status.status === expectedStatus) {
          return;
        }
      }

      // Wait before next poll
      await new Promise(resolve => setTimeout(resolve, pollIntervalMs));
    }

    throw new Error(
      `Timeout waiting for task ${taskId} to reach status "${expectedStatus}" after ${timeoutMs}ms`
    );
  };

  const waitForTaskCompletion = async (taskId: number): Promise<void> => {
    await waitForTaskStatus(taskId, 'COMPLETED', 600000);
  };

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   These tests use waitForTaskStatus which polls until a task reaches IN_PROGRESS,
   *   COMPLETED, or FAILED status. With mock Docker, containers don't actually run,
   *   so tasks never transition past the initial state, causing the tests to timeout.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Ensure Docker is running and can pull the rover agent image
   *   3. Ensure a valid AI agent (Claude CLI, Gemini CLI, etc.) is installed and authenticated
   *   4. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "successful task execution"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('successful task execution', () => {
    // This test requires a real agent to make progress in the container.
    // The mock Docker doesn't run real containers, so tasks never reach COMPLETED status.
    it('should execute a simple task to create a hello world bash script', async () => {
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh that prints the current date and time. It should explicitly print "Hello World" (without quotes and with the exact provided case)'
      );
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }
      expect(result.exitCode).toBe(0);
      await waitForTaskCompletion(1);
      expect(
        existsSync(join(testDir, '.rover/tasks/1/workspace/hello.sh'))
      ).toBe(true);
      const scriptContent = readFileSync(
        join(testDir, '.rover/tasks/1/workspace/hello.sh'),
        'utf8'
      );
      expect(scriptContent).toContain('Hello World');
      expect(scriptContent).toContain('date');
    });

    it('should create a git worktree for task isolation', async () => {
      // Execute: Run rover task
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh that prints the current system user'
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded
      expect(result.exitCode).toBe(0);

      // Wait for task to reach IN_PROGRESS status
      await waitForTaskStatus(1, 'IN_PROGRESS', 600000);

      // Verify: Worktree was created (check git worktree list)
      const worktreeResult = await execa('git', ['worktree', 'list'], {
        cwd: testDir,
      });

      // Should have at least 2 worktrees (main + task worktree)
      const worktreeLines = worktreeResult.stdout
        .split('\n')
        .filter(line => line.trim());
      expect(worktreeLines.length).toBeGreaterThanOrEqual(2);
    });

    it('should create separate tasks when multiple --agent flags are provided', async () => {
      // Execute: Run rover task with multiple agents
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--agent', 'claude', '--agent', 'gemini']
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded
      expect(result.exitCode).toBe(0);

      // Wait for both tasks to reach IN_PROGRESS status in parallel
      await Promise.all([
        waitForTaskStatus(1, 'IN_PROGRESS', 600000),
        waitForTaskStatus(2, 'IN_PROGRESS', 600000),
      ]);

      // Verify: Task 1 description file exists
      const task1Path = join(testDir, '.rover/tasks/1/description.json');
      expect(existsSync(task1Path)).toBe(true);
      const task1Data = JSON.parse(readFileSync(task1Path, 'utf8'));
      expect(task1Data.id).toBe(1);
      expect(task1Data.agent).toBe('claude');

      // Verify: Task 2 description file exists
      const task2Path = join(testDir, '.rover/tasks/2/description.json');
      expect(existsSync(task2Path)).toBe(true);
      const task2Data = JSON.parse(readFileSync(task2Path, 'utf8'));
      expect(task2Data.id).toBe(2);
      expect(task2Data.agent).toBe('gemini');

      // Verify: Both tasks have separate workspaces
      expect(existsSync(join(testDir, '.rover/tasks/1/workspace'))).toBe(true);
      expect(existsSync(join(testDir, '.rover/tasks/2/workspace'))).toBe(true);

      // Verify: Both tasks have separate git branches
      const worktreeResult = await execa('git', ['worktree', 'list'], {
        cwd: testDir,
      });
      const worktreeLines = worktreeResult.stdout
        .split('\n')
        .filter(line => line.trim());
      // Should have at least 3 worktrees (main + task1 + task2)
      expect(worktreeLines.length).toBeGreaterThanOrEqual(3);
    });
  });

  describe('error handling', () => {
    it('should fail gracefully if AI agent is not available', async () => {
      // Setup: Create a failing claude mock to simulate missing AI agent
      createMockTool('claude', 127, 'command not found: claude');

      // Execute: Run rover task
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh that prints the current date and time. It should explicitly print "Hello World" (without quotes and with the exact provided case)'
      );

      // Verify: Command failed with appropriate error
      expect(result.exitCode).not.toBe(0);
      const output = (result.stdout || result.stderr).toLowerCase();
      expect(output).toMatch(/agent|claude|not found|error/);
    });

    it('should create a task even without prior rover init', async () => {
      // Setup: Remove rover configuration to simulate uninitialized project
      rmSync(join(testDir, 'rover.json'), { force: true });
      rmSync(join(testDir, '.rover'), { recursive: true, force: true });

      // Execute: Run rover task without initialization
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--json']
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded - rover init is not required for task creation
      expect(result.exitCode).toBe(0);

      // Verify: JSON output confirms task creation
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(true);
      expect(jsonOutput.taskId).toBe(1);
      expect(jsonOutput.title).toBeTruthy();
    });

    it('should reset task to NEW status when Docker container creation fails', async () => {
      // Setup: Create a failing docker mock that succeeds for 'info' but fails for 'create'
      const dockerScript = `#!/usr/bin/env bash
if [[ "$1" == "info" ]]; then
  echo '{"ServerVersion": "20.10.0"}'
  exit 0
elif [[ "$1" == "create" ]]; then
  echo "Error: docker create failed" >&2
  exit 1
else
  exit 0
fi
`;
      const dockerPath = join(mockBinDir, 'docker');
      writeFileSync(dockerPath, dockerScript);
      chmodSync(dockerPath, 0o755);

      // Execute: Run rover task
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh'
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded (task was created, even though container failed)
      expect(result.exitCode).toBe(0);

      // Verify: Warning about container failure is shown
      const output = result.stdout + (result.stderr || '');
      expect(output).toMatch(/reset to 'New'|error running the container/i);

      // Verify: Restart suggestion is provided
      expect(output).toMatch(/rover restart/);
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Task isolation tests verify that task execution in worktrees doesn't affect
   *   the main branch. This requires real agent completion which doesn't happen
   *   with mock Docker.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Ensure Docker is running and can pull the rover agent image
   *   3. Ensure a valid AI agent (Claude CLI, Gemini CLI, etc.) is installed and authenticated
   *   4. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "task isolation"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('task isolation', () => {
    it('should not affect the main branch during task execution', async () => {
      const initialLog = await execa('git', ['log', '--oneline'], {
        cwd: testDir,
      });
      const initialCommitCount = initialLog.stdout.split('\n').length;
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh that prints the current date and time. It should explicitly print "Hello World" (without quotes and with the exact provided case)'
      );
      expect(result.exitCode).toBe(0);
      await waitForTaskCompletion(1);
      const finalLog = await execa('git', ['log', '--oneline'], {
        cwd: testDir,
      });
      const finalCommitCount = finalLog.stdout.split('\n').length;
      expect(finalCommitCount).toBe(initialCommitCount);
      const branchResult = await execa('git', ['branch', '--show-current'], {
        cwd: testDir,
      });
      expect(branchResult.stdout.trim()).toMatch(/main|master/);
    });
  });

  describe('non-interactive mode', () => {
    it('should produce structured JSON output with --json flag', async () => {
      // Execute: Run rover task with --json flag
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--json']
      );

      // Verify: Command succeeded
      expect(result.exitCode).toBe(0);

      // Verify: Output is valid JSON
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(true);
      expect(jsonOutput.taskId).toBe(1);
      expect(jsonOutput.title).toBeTruthy();
      expect(jsonOutput.description).toBeTruthy();
      expect(jsonOutput.status).toBeTruthy();
      expect(jsonOutput.createdAt).toBeTruthy();
      expect(jsonOutput.workspace).toBeTruthy();
      expect(jsonOutput.branch).toBeTruthy();
    });

    it('should be fully automatable with both --yes and --json flags', async () => {
      // Execute: Run rover task with both --yes and --json flags
      // The -y flag is already passed by runRoverTask, so we only add --json
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--json']
      );

      // Verify: Command succeeded without any interactive prompts
      expect(result.exitCode).toBe(0);

      // Verify: Output is valid JSON containing task metadata
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(true);
      expect(typeof jsonOutput.taskId).toBe('number');
      expect(typeof jsonOutput.title).toBe('string');
      expect(typeof jsonOutput.description).toBe('string');
      expect(typeof jsonOutput.workspace).toBe('string');
      expect(typeof jsonOutput.branch).toBe('string');
      expect(typeof jsonOutput.savedTo).toBe('string');
    });
  });

  describe('source and target branch control', () => {
    it('should use --source-branch to base the worktree on a specific branch', async () => {
      // Setup: Commit rover config files so they're available on all branches
      await execa('git', ['add', '.'], { cwd: testDir });
      await execa('git', ['commit', '-m', 'Add rover config'], {
        cwd: testDir,
      });

      // Create a feature branch with a distinct file
      await execa('git', ['checkout', '-b', 'feature-base'], { cwd: testDir });
      writeFileSync(join(testDir, 'feature-file.txt'), 'feature content\n');
      await execa('git', ['add', 'feature-file.txt'], { cwd: testDir });
      await execa('git', ['commit', '-m', 'Add feature file'], {
        cwd: testDir,
      });

      // Switch back to main branch
      await execa('git', ['checkout', 'master'], { cwd: testDir }).catch(() =>
        execa('git', ['checkout', 'main'], { cwd: testDir })
      );

      // Execute: Run rover task with --source-branch pointing to the feature branch
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--source-branch', 'feature-base', '--json']
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded
      expect(result.exitCode).toBe(0);

      // Verify: JSON output confirms task creation
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(true);
      expect(jsonOutput.taskId).toBe(1);

      // Verify: The task workspace contains the file from the feature branch
      const workspacePath = jsonOutput.workspace;
      expect(existsSync(join(workspacePath, 'feature-file.txt'))).toBe(true);
      const content = readFileSync(
        join(workspacePath, 'feature-file.txt'),
        'utf8'
      );
      expect(content).toContain('feature content');
    });

    it('should use --target-branch to set a custom branch name for the task', async () => {
      const customBranchName = 'custom-task-branch';

      // Execute: Run rover task with --target-branch
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--target-branch', customBranchName, '--json']
      );

      // Debug output if test fails
      if (result.exitCode !== 0) {
        console.log('STDOUT:', result.stdout);
        console.log('STDERR:', result.stderr);
      }

      // Verify: Command succeeded
      expect(result.exitCode).toBe(0);

      // Verify: JSON output shows the custom branch name
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(true);
      expect(jsonOutput.branch).toBe(customBranchName);

      // Verify: The custom branch exists in the git repository
      const branchResult = await execa(
        'git',
        ['branch', '--list', customBranchName],
        {
          cwd: testDir,
        }
      );
      expect(branchResult.stdout).toContain(customBranchName);
    });

    it('should fail when --source-branch specifies a non-existent branch', async () => {
      // Execute: Run rover task with a non-existent source branch
      const result = await runRoverTask(
        'Create a hello world bash script named hello.sh',
        ['--source-branch', 'non-existent-branch', '--json']
      );

      // Verify: Command failed
      expect(result.exitCode).not.toBe(0);

      // Verify: JSON output contains error about the branch
      const jsonOutput = JSON.parse(result.stdout);
      expect(jsonOutput.success).toBe(false);
      expect(jsonOutput.errors).toBeDefined();
      expect(
        jsonOutput.errors.some((e: string) => e.includes('non-existent-branch'))
      ).toBe(true);
    });
  });
});

/**
 * ============================================================================
 * AUTOGENERATED E2E TEST FILE - DO NOT MODIFY MANUALLY
 * ============================================================================
 *
 * To modify these tests, update the specification files:
 * - /docs/E2E_TESTS.md (root-level testing philosophy and constraints)
 * - /packages/cli/E2E_TESTS.md (CLI-specific test specifications)
 *
 * Then run the /update-e2e-tests command to regenerate this file.
 * ============================================================================
 */
