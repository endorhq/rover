/**
 * ============================================================================
 * AUTOGENERATED E2E TEST FILE - DO NOT MODIFY MANUALLY
 * ============================================================================
 *
 * This file is automatically generated based on the E2E test specifications.
 * Any manual changes will be overwritten by the automation.
 *
 * To modify these tests, update the specification files:
 * - /docs/E2E_TESTS.md (root-level testing philosophy and constraints)
 * - /packages/cli/E2E_TESTS.md (CLI-specific test specifications)
 *
 * Then run the /update-e2e-tests command to regenerate this file.
 * ============================================================================
 */

import { beforeEach, afterEach, describe, it, expect } from 'vitest';
import { SKIP_REAL_AGENT_TESTS } from './e2e-utils.js';
import {
  mkdtempSync,
  rmSync,
  writeFileSync,
  readFileSync,
  existsSync,
  mkdirSync,
  chmodSync,
} from 'node:fs';
import { tmpdir } from 'node:os';
import { join } from 'node:path';
import { execa } from 'execa';

/**
 * E2E tests for Rover hooks system
 *
 * These tests run the actual rover CLI binary and test the hook execution workflow.
 * Hooks are configured in rover.json and execute shell commands when task lifecycle
 * events occur (onComplete, onMerge, onPush).
 */

describe('rover hooks (e2e)', () => {
  let testDir: string;
  let remoteDir: string;
  let originalCwd: string;
  let mockBinDir: string;
  let mockHomeDir: string;
  let originalPath: string;

  const createMockTool = (
    toolName: string,
    exitCode: number = 0,
    output: string = 'mock version 1.0.0'
  ) => {
    const scriptPath = join(mockBinDir, toolName);
    const scriptContent = `#!/usr/bin/env bash\necho "${output}"\nexit ${exitCode}`;
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  const createMockScript = (toolName: string, scriptContent: string) => {
    const scriptPath = join(mockBinDir, toolName);
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  const createMockClaude = () => {
    createMockScript(
      'claude',
      `#!/usr/bin/env bash
if [[ "$1" == "--version" ]]; then
  echo "Claude CLI v1.0.0"
  exit 0
fi

if [[ "$1" == "-p" ]]; then
  PROMPT=$(cat)
  if [[ "$2" == "--output-format" && "$3" == "json" ]]; then
    echo '{"result":"{\\"title\\":\\"Test task\\",\\"description\\":\\"A test task description.\\"}"}'
  else
    echo '{"title":"Test task","description":"A test task description."}'
  fi
  exit 0
fi

echo "Claude CLI v1.0.0"
exit 0
`
    );
  };

  const roverBin = join(__dirname, '../../../dist/index.mjs');

  const runRover = async (args: string[]) => {
    const testPath = `${mockBinDir}:${originalPath}`;
    return execa('node', [roverBin, ...args], {
      cwd: testDir,
      env: {
        PATH: testPath,
        HOME: mockHomeDir,
        USER: process.env.USER,
        TMPDIR: process.env.TMPDIR,
        ROVER_NO_TELEMETRY: '1',
      },
      reject: false,
    });
  };

  const waitForTaskCompletion = async (
    taskId: number,
    timeoutMs: number = 600000
  ): Promise<string> => {
    const startTime = Date.now();
    while (Date.now() - startTime < timeoutMs) {
      const result = await runRover(['inspect', String(taskId), '--json']);
      if (result.exitCode === 0) {
        const output = JSON.parse(result.stdout);
        if (output.status === 'COMPLETED' || output.status === 'FAILED')
          return output.status;
      }
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    throw new Error(`Timeout waiting for task ${taskId} to complete`);
  };

  beforeEach(async () => {
    originalCwd = process.cwd();
    originalPath = process.env.PATH || '';

    // Create a bare remote repository for push tests
    remoteDir = mkdtempSync(join(tmpdir(), 'rover-hooks-remote-'));
    await execa('git', ['init', '--bare', remoteDir]);

    testDir = mkdtempSync(join(tmpdir(), 'rover-hooks-e2e-'));
    process.chdir(testDir);

    mockBinDir = join(testDir, '.mock-bin');
    mkdirSync(mockBinDir, { recursive: true });

    // Create mock HOME directory with agent configuration files
    mockHomeDir = join(testDir, '.mock-home');
    mkdirSync(mockHomeDir, { recursive: true });
    writeFileSync(
      join(mockHomeDir, '.claude.json'),
      JSON.stringify({ version: 1 })
    );

    process.env.PATH = `${mockBinDir}:${originalPath}`;

    createMockTool('docker', 127, 'command not found: docker');
    createMockTool('claude', 127, 'command not found: claude');
    createMockTool('codex', 127, 'command not found: codex');
    createMockTool('cursor', 127, 'command not found: cursor');
    createMockTool('cursor-agent', 127, 'command not found: cursor-agent');
    createMockTool('gemini', 127, 'command not found: gemini');
    createMockTool('qwen', 127, 'command not found: qwen');
    createMockTool('opencode', 127, 'command not found: opencode');

    createMockTool('docker', 0, 'Docker version 24.0.0');
    createMockClaude();

    await execa('git', ['init']);
    await execa('git', ['config', 'user.email', 'test@test.com']);
    await execa('git', ['config', 'user.name', 'Test User']);
    await execa('git', ['config', 'commit.gpgsign', 'false']);

    // Add remote
    await execa('git', ['remote', 'add', 'origin', remoteDir], {
      cwd: testDir,
    });

    writeFileSync(
      'package.json',
      JSON.stringify(
        { name: 'test-project', version: '1.0.0', type: 'module' },
        null,
        2
      )
    );
    writeFileSync('README.md', '# Test Project\n');

    await execa('git', ['add', '.']);
    await execa('git', ['commit', '-m', 'Initial commit']);
    await execa('git', ['push', '-u', 'origin', 'master'], {
      cwd: testDir,
    }).catch(() =>
      execa('git', ['push', '-u', 'origin', 'main'], { cwd: testDir })
    );

    await runRover(['init', '--yes']);
  });

  afterEach(() => {
    process.chdir(originalCwd);
    process.env.PATH = originalPath;
    rmSync(testDir, { recursive: true, force: true });
    rmSync(remoteDir, { recursive: true, force: true });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Hook tests (onComplete) require tasks to reach terminal states (COMPLETED, FAILED).
   *   With mock Docker, containers don't execute, so hooks are never triggered.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Ensure Docker is running and can pull the rover agent image
   *   3. Ensure a valid AI agent (Claude CLI, Gemini CLI, etc.) is installed and authenticated
   *   4. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "onComplete hook"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('onComplete hook', () => {
    it('should execute onComplete hook when task reaches terminal status', async () => {
      const hookLogFile = join(testDir, 'complete-hook.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onComplete: [
          `echo "TASK_ID=$ROVER_TASK_ID STATUS=$ROVER_TASK_STATUS" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);

      // Allow some time for the hook to execute
      await new Promise(resolve => setTimeout(resolve, 1000));

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toContain('TASK_ID=');
      expect(hookLog).toMatch(/STATUS=(COMPLETED|FAILED)/);
    });

    it('should receive ROVER_TASK_ID, ROVER_TASK_BRANCH, ROVER_TASK_TITLE, and ROVER_TASK_STATUS', async () => {
      const hookLogFile = join(testDir, 'complete-hook-vars.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onComplete: [
          `echo "ID=$ROVER_TASK_ID BRANCH=$ROVER_TASK_BRANCH TITLE=$ROVER_TASK_TITLE STATUS=$ROVER_TASK_STATUS" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);

      await new Promise(resolve => setTimeout(resolve, 1000));

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toMatch(/ID=\d+/);
      expect(hookLog).toContain('BRANCH=');
      expect(hookLog).toContain('TITLE=');
      expect(hookLog).toContain('STATUS=');
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   onMerge hook tests require tasks to complete and merge successfully.
   *   With mock Docker, no merges happen.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "onMerge hook"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('onMerge hook', () => {
    it('should execute onMerge hook after successful merge', async () => {
      const hookLogFile = join(testDir, 'merge-hook.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onMerge: [
          `echo "TASK_ID=$ROVER_TASK_ID BRANCH=$ROVER_TASK_BRANCH" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);
      await runRover(['merge', '1', '--json']);

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toContain('TASK_ID=');
      expect(hookLog).toContain('BRANCH=');
    });

    it('should receive ROVER_TASK_ID, ROVER_TASK_BRANCH, and ROVER_TASK_TITLE', async () => {
      const hookLogFile = join(testDir, 'merge-hook-vars.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onMerge: [
          `echo "ID=$ROVER_TASK_ID BRANCH=$ROVER_TASK_BRANCH TITLE=$ROVER_TASK_TITLE" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);
      await runRover(['merge', '1', '--json']);

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toMatch(/ID=\d+/);
      expect(hookLog).toContain('BRANCH=');
      expect(hookLog).toContain('TITLE=');
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   onPush hook tests require tasks to complete and push successfully.
   *   With mock Docker, no pushes happen.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "onPush hook"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('onPush hook', () => {
    it('should execute onPush hook after successful push', async () => {
      const hookLogFile = join(testDir, 'push-hook.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onPush: [
          `echo "TASK_ID=$ROVER_TASK_ID BRANCH=$ROVER_TASK_BRANCH" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);
      await runRover(['push', '1', '--json']);

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toContain('TASK_ID=');
      expect(hookLog).toContain('BRANCH=');
    });

    it('should receive ROVER_TASK_ID, ROVER_TASK_BRANCH, and ROVER_TASK_TITLE', async () => {
      const hookLogFile = join(testDir, 'push-hook-vars.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onPush: [
          `echo "ID=$ROVER_TASK_ID BRANCH=$ROVER_TASK_BRANCH TITLE=$ROVER_TASK_TITLE" > ${hookLogFile}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);
      await runRover(['push', '1', '--json']);

      expect(existsSync(hookLogFile)).toBe(true);
      const hookLog = readFileSync(hookLogFile, 'utf8');
      expect(hookLog).toMatch(/ID=\d+/);
      expect(hookLog).toContain('BRANCH=');
      expect(hookLog).toContain('TITLE=');
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Hook failure isolation tests require hooks to actually execute.
   *   With mock Docker, no hooks are triggered.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "hook failure"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('hook failure isolation', () => {
    it('should not block the operation when a hook command fails', async () => {
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onComplete: ['exit 1', 'nonexistent-command'],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      const status = await waitForTaskCompletion(1);

      // Task should still complete even though hook failed
      expect(['COMPLETED', 'FAILED']).toContain(status);

      // Verify task is still accessible
      const inspectResult = await runRover(['inspect', '1', '--json']);
      expect(inspectResult.exitCode).toBe(0);
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Multiple hook command tests require hooks to actually execute.
   *   With mock Docker, no hooks are triggered.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "multiple hook"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('multiple hook commands', () => {
    it('should execute all commands in the hook array', async () => {
      const hookLogFile1 = join(testDir, 'hook1.log');
      const hookLogFile2 = join(testDir, 'hook2.log');
      const roverConfig = JSON.parse(
        readFileSync(join(testDir, 'rover.json'), 'utf8')
      );
      roverConfig.hooks = {
        onComplete: [
          `echo "FIRST" > ${hookLogFile1}`,
          `echo "SECOND" > ${hookLogFile2}`,
        ],
      };
      writeFileSync(
        join(testDir, 'rover.json'),
        JSON.stringify(roverConfig, null, 2)
      );

      await runRover(['task', '-y', 'Create a hello world script']);
      await waitForTaskCompletion(1);

      await new Promise(resolve => setTimeout(resolve, 1000));

      expect(existsSync(hookLogFile1)).toBe(true);
      expect(existsSync(hookLogFile2)).toBe(true);

      const log1 = readFileSync(hookLogFile1, 'utf8');
      const log2 = readFileSync(hookLogFile2, 'utf8');
      expect(log1).toContain('FIRST');
      expect(log2).toContain('SECOND');
    });
  });
});
